{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lzma\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandarallel import pandarallel\n",
    "from skspatial.objects import Line\n",
    "from scipy.spatial import cKDTree\n",
    "import geopy.distance as distance\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup fase\n",
    "pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_veiculos(path, COD):\n",
    "    file = lzma.open(path)\n",
    "    df = pd.read_json(file, lines=True)\n",
    "    df = df[df[\"COD_LINHA\"] == COD]\n",
    "    df[\"LAT\"] = df[\"LAT\"].apply(lambda x: float(x.replace(',', '.')))\n",
    "    df[\"LON\"] = df[\"LON\"].apply(lambda x: float(x.replace(',', '.')))\n",
    "    df[\"DTHR\"] = pd.to_datetime(\n",
    "    df[\"DTHR\"], format=\"%d/%m/%Y %H:%M:%S\").dt.tz_localize(\"America/Sao_Paulo\")\n",
    "    df['coords'] = df.apply(lambda x: (x['LAT'], x['LON']), axis=1)\n",
    "    df.drop(columns=[\"LAT\", \"LON\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "def load_shape(path, COD):\n",
    "    df = pd.read_json(path)\n",
    "    df = df[df[\"COD\"] == COD]\n",
    "    df[\"LAT\"] = df[\"LAT\"].apply(lambda x: float(x.replace(',', '.')))\n",
    "    df[\"LON\"] = df[\"LON\"].apply(lambda x: float(x.replace(',', '.')))\n",
    "    df['coords'] = df.apply(lambda x: (x['LAT'], x['LON']), axis=1)\n",
    "    df = df.groupby(['COD', 'SHP'])['coords'].apply(list).reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfVeiculos = load_veiculos(\"../../data/veiculos/2023_05_03_veiculos.json.xz\", '022')\n",
    "dfShape = load_shape(\"../../data/shape.json\", \"022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findNearestPoint(vehicle, shape):\n",
    "    nA = np.array(list(vehicle.coords))\n",
    "    nB = np.array(list(shape.coords))\n",
    "\n",
    "    nB = nB.reshape(-1, 2)\n",
    "\n",
    "    btree = cKDTree(nB)\n",
    "    dist, idx = btree.query(nA, k=1)\n",
    "\n",
    "    vehicle['nearest'] = nB[idx].tolist()\n",
    "    vehicle['idx'] = idx / len(nB)\n",
    "\n",
    "    return vehicle\n",
    "\n",
    "dfVeiculos = findNearestPoint(dfVeiculos, dfShape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfVeiculos[\"distances\"] = dfVeiculos.parallel_apply(lambda x: distance.distance(x['coords'], x['nearest']).meters, axis=1)\n",
    "dfVeiculos = dfVeiculos[dfVeiculos[\"distances\"] < 200]\n",
    "\n",
    "dfVeiculos[\"DTHR\"] = dfVeiculos[\"DTHR\"].dt.strftime(\"%H:%M\")\n",
    "\n",
    "dfVeiculos.set_index(\"DTHR\", inplace=True)\n",
    "\n",
    "dfVeiculos[dfVeiculos[\"VEIC\"] == \"BL605\"][\"idx\"].plot().invert_xaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the 'idx' column to be between 0 and 1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dfVeiculos['idx'] = scaler.fit_transform(dfVeiculos['idx'].values.reshape(-1, 1))\n",
    "\n",
    "# Sort the DataFrame by the 'DTHR' column (time)\n",
    "dfVeiculos.sort_values(by='DTHR', inplace=True)\n",
    "\n",
    "# Divide the data into 3 equal parts\n",
    "data_1, data_2, data_3 = np.array_split(dfVeiculos, 3)\n",
    "\n",
    "# Function to create time-series dataset for LSTM\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        X.append(dataset[i:(i + look_back), 0])\n",
    "        Y.append(dataset[i + look_back, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Choose a look_back window size\n",
    "look_back = 3\n",
    "\n",
    "# Prepare the data for each part\n",
    "X1, y1 = create_dataset(data_1['idx'].values, look_back)\n",
    "X2, y2 = create_dataset(data_2['idx'].values, look_back)\n",
    "X3, y3 = create_dataset(data_3['idx'].values, look_back)\n",
    "\n",
    "# Combine the data from all parts\n",
    "X = np.concatenate((X1, X2, X3))\n",
    "y = np.concatenate((y1, y2, y3))\n",
    "\n",
    "# Reshape the input to be [samples, time steps, features]\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(look_back, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=1, verbose=2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Invert the scaling to get the original values\n",
    "y_train_pred = scaler.inverse_transform(y_train_pred)\n",
    "y_test_pred = scaler.inverse_transform(y_test_pred)\n",
    "y_train = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-pre-processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
